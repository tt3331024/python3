{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pySpark ml word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定spark session 環境變數 spark home\n",
    "# 詳細 ?findspark.init\n",
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "# 載入必要modules/\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.ml.feature import Word2Vec, Word2VecModel\n",
    "\n",
    "hdfs_path = \"hdfs://your_ip:port\"\n",
    "coresMax = \"4\"\n",
    "executorMem = \"12g\"\n",
    "\n",
    "# 開啟 spark session\n",
    "# pyspark session 目前需要手動設定使用到的參數\n",
    "# spark.executor.memory worker 計算/cache相關記憶體大小\n",
    "# spark.driver.memory 與 driver 相關的記憶體大小影響task分派，對演算速度影響大\n",
    "# spark.driver.maxResultSize 設定分散資料collect回本地端的大小上限，SparkR有更改設定檔無限制，此處須設置。\n",
    "ss = SparkSession.builder \\\n",
    "    .master( \"spark://your_ip:port\" ) \\\n",
    "    .appName( \"word_to_vector\" ) \\\n",
    "    .config( \"spark.cores.max\", coresMax ) \\\n",
    "    .config( \"spark.executor.memory\", executorMem ) \\\n",
    "    .config( \"spark.driver.memory\", \"20g\" ) \\\n",
    "    .config( \"spark.driver.maxResultSize\", \"20g\" ) \\\n",
    "    .config( \"spark.rpc.message.maxSize\", \"512\") \\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- source: string (nullable = true)\n",
      " |-- categroy: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 讀取 parquet 資料源\n",
    "# 使用資料 wiki, 一些 ppt, 電商, 部落格的文章\n",
    "corpus = ss.read.parquet( hdfs_path + \"/your_path/*.parquet\" )\n",
    "corpus.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------------------+-------------+--------------------+\n",
      "|source|categroy|                 url|        title|               words|\n",
      "+------+--------+--------------------+-------------+--------------------+\n",
      "|  wiki|        |https://zh.wikipe...|   浸信宣道會呂明才小學|[浸信, 宣道, 呂明, 小學, ...|\n",
      "|  wiki|        |https://zh.wikipe...|        索拉庫喬山|[索拉, 庫喬, 索拉, 庫喬, ...|\n",
      "|  wiki|        |https://zh.wikipe...|阿南塔山 (阿雷基帕大區)|[阿南, 塔山, 阿雷基帕, 大區...|\n",
      "+------+--------+--------------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010510"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由於是每天爬文，會有少量文章重複，因此要刪除重複的文章\n",
    "corpus = corpus.dropDuplicates()\n",
    "corpus.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 word2vec 參數，並進行訓練和儲存模型\n",
    "# numPartitions 使用和 core 數相同，maxIter 數官方文件建議須小於等於 partition 數\n",
    "# maxSentenceLength 設定為 8000，我們 corpus 的長度都超過 1000，預設數值明顯不符合需求。需要確認文章長度平均值及測試。\n",
    "word2Vec = Word2Vec( vectorSize = 240, \n",
    "                     inputCol = \"words\", \n",
    "                     outputCol = \"word2vec\",\n",
    "                     windowSize = 6,\n",
    "                     numPartitions = 8,\n",
    "                     stepSize = 0.05, \n",
    "                     maxIter = 8,\n",
    "                     maxSentenceLength = 8000 )\n",
    "w2v_model = word2Vec.fit( corpus )\n",
    "w2v_model.write().overwrite().save( hdfs_path + \"/your_path/WordToVector_model.ml\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|     word|              vector|\n",
      "+---------+--------------------+\n",
      "|       凍人|[10852.51171875,-...|\n",
      "|       克幸|[102047.2578125,-...|\n",
      "|hoshizora|[1119.98083496093...|\n",
      "|      王義芳|[1990.271484375,-...|\n",
      "|       焦尼|[11.6127252578735...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getVectors() 方法是取得 w2v 的 embedding，其中 word 是字，vector 是該字長為 240 的 embedding\n",
    "w2v_model.getVectors().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出 word embedding 另外儲存，供後續 sparkR 分析使用，由於 sparkR 不能針對 vector 進行分析，因此先轉為 array 在儲存\n",
    "w2v_vectors = w2v_model.getVectors()\n",
    "w2v_embedding = w2v_vectors.rdd.map(lambda row: (row[0], row[1].toArray().tolist()) ).toDF(schema=['word', 'embedding'])\n",
    "w2v_embedding.write.parquet( path = hdfs_path + \"/your_path/WordToVector_embedding.parquet\", \n",
    "                             mode = \"overwrite\",\n",
    "                             compression = \"gzip\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|word|        similarity|\n",
      "+----+------------------+\n",
      "|  王子|0.6979262828826904|\n",
      "|伊莉莎白|0.6587029099464417|\n",
      "|瑪格麗特|0.6476824283599854|\n",
      "|伊麗莎白|0.6196021437644958|\n",
      "|  亨利|0.6115877032279968|\n",
      "|  王后|0.6103411912918091|\n",
      "|  國王|0.6059281826019287|\n",
      "|  二世|0.5922608971595764|\n",
      "|  瑪麗|0.5908293724060059|\n",
      "|  安娜|0.5880428552627563|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 嘗試 word2vec 訓練成效\n",
    "w2v_model.findSynonyms( \"女王\", 10 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 須先啟動 spark session 才能讀取模型\n",
    "from pyspark.ml.feature import Word2VecModel\n",
    "w2v_model = Word2VecModel.load( hdfs_path + \"/your_path/WordToVector_model.ml\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
