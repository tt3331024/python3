{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import time\n",
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "# 載入必要module\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import types\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "# 開啟 spark session\n",
    "# pyspark session 目前需要手動設定使用到的參數\n",
    "# spark.executor.memory worker 計算/cache相關記憶體大小\n",
    "# spark.driver.memory 與 driver 相關的記憶體大小影響task分派，對演算速度影響大\n",
    "# spark.driver.maxResultSize 設定分散資料collect回本地端的大小上限，SparkR有更改設定檔無限制，此處須設置。\n",
    "ss = SparkSession.builder \\\n",
    "    .master( \"spark://192.168.1.52:7077\" ) \\\n",
    "    .appName( \"talk.tw LDA\" ) \\\n",
    "    .config( \"spark.cores.max\", \"4\" ) \\\n",
    "    .config( \"spark.executor.memory\", \"12g\" ) \\\n",
    "    .config( \"spark.driver.memory\", \"24g\" ) \\\n",
    "    .config( \"spark.driver.maxResultSize\", \"16g\" ) \\\n",
    "    .getOrCreate() \n",
    "# 讀取資料並整理\n",
    "talk_tw = ss.read.csv( path = \"hdfs://192.168.1.53:9000/corpus/ATS/talk.tw/20180905.tsv.gz\", \n",
    "                       sep = \"\\t\" )\n",
    "talk_tw = talk_tw.toDF( 'category','url','author','tag','createTime','title','text')\n",
    "talk_tw = talk_tw.where(\"url IS NOT NULL\").where(\"text IS NOT NULL\")\n",
    "talk_tw = talk_tw.repartition(4)\n",
    "# 匯入停用詞\n",
    "stopWord_set = set()\n",
    "stopWords_path = '/home/stat_jerry/stop_word_test1800.txt'\n",
    "with open( stopWords_path, 'r', encoding='utf-8') as stopwords:\n",
    "    for stopword in stopwords:\n",
    "        stopWord_set.add(stopword.strip('\\n'))\n",
    "# 設定 UDF 使用 jieba 斷詞，並依停用詞來篩選。最後輸出格式為 array, 內容為 str\n",
    "def cut_stopWord(w):\n",
    "    l = []\n",
    "    seg = jieba.cut(w)\n",
    "    for word in seg:\n",
    "        if word not in stopWord_set:\n",
    "            if not word.isdigit():\n",
    "                if len( word ) > 1:\n",
    "                    l.append(word)\n",
    "    return l\n",
    "cut_udf = f.udf( cut_stopWord, types.ArrayType(types.StringType()) )\n",
    "# 斷詞\n",
    "talk_tw_cut = talk_tw.select( 'category', 'url', 'author', 'tag', 'title', cut_udf('text').alias('words'), 'createTime' )\n",
    "talk_tw_cut = talk_tw_cut.where(\"words[0] IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 先將文字向量化\n",
    "cv = CountVectorizer( vocabSize=2**20, inputCol='words', outputCol='features')\n",
    "cv_model = cv.fit(talk_tw_cut)\n",
    "result = cv_model.transform(talk_tw_cut)\n",
    "# 訓練 LDA 模型\n",
    "lda = LDA( featuresCol=\"features\", k=15, maxIter=100, learningOffset=1024, learningDecay=0.7, seed=110, optimizer=\"em\" )\n",
    "lda_model = lda.fit(result)\n",
    "lda_result = lda_model.transform(result)\n",
    "# 存取LDA模型\n",
    "lda_model.write().overwrite().save( \"hdfs://192.168.1.53:9000/corpus/ml/LDA_model_20180905\" )\n",
    "cv_model.write().overwrite().save( \"hdfs://192.168.1.53:9000/corpus/ml/CountVectorizer_model_20180905\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 計算 pyLDAvis 所需的資料\n",
    "# topic_term_dists\n",
    "ttd = (lda_model.topicsMatrix().toArray().T / lda_model.topicsMatrix().toArray().T.sum(axis=1)[:, None]).tolist()\n",
    "# doc_topic_dists\n",
    "dtd = lda_result.rdd.map(lambda x: x['topicDistribution'].values).map(lambda y: [i for i in y]).collect()\n",
    "# term_frequency\n",
    "tf = result[['features']].rdd.map(lambda row: row['features'].toArray()).reduce(lambda x,y: [x[i]+y[i] for i in range(len(y))])\n",
    "# doc_lengths\n",
    "count_udf = f.udf( lambda x: x.numNonzeros(), types.IntegerType() )\n",
    "dl = result.select( count_udf('features').alias('doc_lengths')).rdd.flatMap(lambda x: x).collect()\n",
    "# vocab\n",
    "v = cv_model.vocabulary\n",
    "\n",
    "# 使用 pyLDAvis 繪圖 \n",
    "ldavis = pyLDAvis.prepare( topic_term_dists=ttd, doc_topic_dists=dtd, doc_lengths=dl, vocab=v, term_frequency=tf,  sort_topics=False )\n",
    "pyLDAvis.save_html(ldavis, '/home/stat_jerry/talk_LDAvis_0905.html')\n",
    "\n",
    "# 將資料存回本地並計算個文章最高的 topic 是哪個\n",
    "doc_topic_pd = lda_result.select('url','title','topicDistribution').toPandas()\n",
    "topic_df = pd.DataFrame()\n",
    "for i in range(doc_topic_pd.shape[0]):\n",
    "    tmp = pd.DataFrame(doc_topic_pd['topicDistribution'][i].values)\n",
    "    topic_df = pd.concat([topic_df, tmp], axis=1)\n",
    "topic_df = topic_df.T\n",
    "topic_df.columns = [\"topic_\" + str(i) for i in range(1,16)]\n",
    "topic_df = topic_df.reset_index(drop=True)\n",
    "dominant_topic = np.argmax(topic_df.values, axis=1)\n",
    "doc_topic_pd['dominant_topic'] = (dominant_topic + 1)\n",
    "doc_topic = pd.concat( [doc_topic_pd[['url', 'dominant_topic']], topic_df], axis=1)\n",
    "ss.createDataFrame(doc_topic)\\\n",
    "  .repartition(1)\\\n",
    "  .write.csv( path = \"hdfs://192.168.1.53:9000/ATS/Preprocess_Data/2018M08/docs_topic_table/talk_tw.csv\",\n",
    "              mode = \"overwrite\",\n",
    "              compression = \"gzip\" )\n",
    "# 將所有主題 top 30 個字回傳成 list，供後續\n",
    "topics = lda_model.describeTopics(30)\n",
    "topics_rdd = topics.rdd\n",
    "pd.DataFrame(topics_rdd.map(lambda row: row['termIndices'])\\\n",
    "             .flatMap(lambda idx_list: [v[idx] for idx in idx_list])\\\n",
    "             .collect()).to_csv(\"LDA_kw_top30_0906.csv\", header = None, index = None)\n",
    "ss.stop()\n",
    "\n",
    "# # 讀取模型的用法\n",
    "# from pyspark.ml.clustering import DistributedLDAModel\n",
    "# from pyspark.ml.feature import CountVectorizerModel\n",
    "# lda_model_2 = DistributedLDAModel.load( \"hdfs://192.168.1.53:9000/corpus/ml/LDA_model_20180906\" )\n",
    "# cv_model_2 = CountVectorizerModel.load(\"hdfs://192.168.1.53:9000/corpus/ml/CountVectorizer_model_20180906\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
