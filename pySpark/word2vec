# 設定spark session 環境變數 spark home
# 詳細 ?findspark.init
import findspark
findspark.init('/usr/local/spark')
# 載入必要module
from pyspark.sql import SparkSession
import pyspark.sql.functions as Func
from pyspark.ml.feature import Word2Vec
# 開啟 spark session
# pyspark session 目前需要手動設定使用到的參數
# spark.executor.memory worker 計算/cache相關記憶體大小
# spark.driver.memory 與 driver 相關的記憶體大小影響task分派，對演算速度影響大
# spark.driver.maxResultSize 設定分散資料collect回本地端的大小上限，SparkR有更改設定檔無限制，此處須設置。
ss = SparkSession.builder \
    .master( "spark://your_ip" ) \
    .appName( "word_to_vector" ) \
    .config( "spark.cores.max", "2" ) \
    .config( "spark.executor.memory", "4g" ) \
    .getOrCreate() 
# 確認 spark session 設定情況
print( ss.sparkContext.getConf().getAll() )
wiki_df = ss.read.csv( path = "hdfs://your_ip/corpus/temp/temp_wiki_20180728",
                          sep = "\t" )
wiki_df = wiki_df.toDF( "Resource", "Index", "Title", "Keywords" )

# 將 string type column 轉換成根據','切割的 array type column
wiki_df_train = wiki_df.select( "Keywords" )
wiki_df_train = wiki_df_train.withColumn( colName = "Keywords", 
                                          col = pyspark.sql.functions.split( str = wiki_df_train.Keywords, pattern = "," ) )
# 設定 word to vector 的演算法參數
# 一個字詞轉換成長度 100 的向量
# 使用 skip-gram 演算法，移動的窗口大小為5，會考慮前後 5 的範圍內預測字詞
# 切成 4 個partition演算，因為使用 4 個core，迭代次數設定4-根據官方文件須小於等於partition數
# 學習速率設定為0.05
# 句子長度設定為 5000，抽樣看一篇維基百科文章有1000~3000，預設1000不符合需求。需要確認文章長度平均值及測試。
word2Vec = Word2Vec( vectorSize = 100, 
                     inputCol = "Keywords", 
                     outputCol = "model",
                     windowSize = 5,
                     numPartitions = 4,
                     stepSize = 0.05, 
                     maxIter = 4,
                     maxSentenceLength = 5000 )
# 訓練 word to vector模型
model = word2Vec.fit( wiki_df_train )
# 儲存模型，因訓練費時，注意不要覆蓋現有結果
model.save( "hdfs://your_ip/corpus/ml/word_to_vector_model" )

# 使用既有結果
# 讀取模型
from pyspark.ml.feature import Word2VecModel
model = Word2VecModel.load( "hdfs://your_ip/corpus/ml/word_to_vector_model" )
# 查看訓練出來的前幾筆詞彙與向量
model.getVectors().show()
# 根據輸入的詞彙回傳相似度最高的前n個詞彙，此處範例為回傳房地產最相關的20個詞。
from pyspark.sql.functions import format_number as fmt
model.findSynonyms( "房地產", 20 ).select( "word", fmt( "similarity", 5 ).alias( "similarity" ) ).show()


